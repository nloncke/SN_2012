\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}

\begin{document}

\title{Unit Testing our Python Library}
\date{\today}
\maketitle

\textsc{Here are} our unit tests for our research-related modules,
pca.py and resids.py.

\tableofcontents

\newpage

\section{\texttt{pca.partial\_sums()}}
\label{sec:partialsums}

We used the matrices
\[coeffs = \left[ \begin{array}{ccc}
4 & 1 & 3 \\
-1 & 2 & 5 \\
10 & -1 & 4 \\
2 & 7 & -9 \\
3 & -3 & 8 \\
6 & 6 & -1
\end{array} \right] \]
and
\[bvecs = \left[ \begin{array}{ccccc}
1 & -2 & 3 & 4 & -5 \\
-6 & 7 & 0 & 9 & 10 \\
8 & -1 & 2 & -2 & 3
\end{array} \right] \]

for our coefficients and basis vectors, respectively.

\subsection{Expected output}
\label{sec:manual}

We manually calculated that our \verb|pca.partial_sums()| function
ought to output a three-dimensional array as follows when passed
coeffs and basisvecs:

\[
\left[ \begin{array}[c]{c}

\left[ \begin{array}{ccccc}
4 & -8 & 12 & 16 & -20 \\
-1 & 2 & -3 & -4 & 5 \\
0 & 0 & 0 & 0 & 0 \\
2 & -4 & 6 & 8 & -10 \\
3 & -6 & 9 & 12 & -15 \\
6 & -12 & 18 & 24 & -30
\end{array} \right] \\

\\

\left[ \begin{array}{ccccc}
-2 & -1 & 12 & 25 & -10 \\
-13 & 16 & -3 & 14 & 25 \\
6 & -7 & 0 & -9 & -10 \\
-40 & 45 & 6 & 71 & 60 \\
21 & -27 & 9 & -15 & -45 \\
-30 & 30 & 18 & 78 & 30
\end{array} \right] \\

\\

\left[ \begin{array}{ccccc}
22 & -4 & 18 & 19 & -1 \\
27 & 11 & 7 & 4 & 40 \\
38 & -4 & 8 & -17 & 2 \\
-112 & 54 & -12 & 89 & 33 \\
85 & -35 & 25 & -31 & -21 \\
-38 & 31 & 20 & 76 & 33
\end{array} \right]

\end{array}\right].
\]

\subsection{Code Issues}
\label{sec:coding}

On both of our machines we found that the function was barfing a
ValueError due to the shape that we set for the result in our script.

\begin{verbatim}
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call
last)
/home/nicole/Documents/summerprog/hsiao_spectra/<ipython-input-263-41a4b465b960>
in <module>()
----> 1 pca.partial_sums(c1, bvecs)

/home/nicole/Documents/summerprog/hsiao_spectra/pca.py in
partial_sums(coeffs, basisvecs)
     59     result_shape = (basisvecs.shape[0],) + coeffs.shape
     60     result = np.zeros(result_shape)
---> 61     result[0,:,:] = nth_pca_term(0, coeffs, basisvecs)
     62     for n in range(1, len(result)):
     63         result[n,:,:] = (result[n-1,:,:] +

ValueError: operands could not be broadcast together with shapes (6,3)
(6,5) 
\end{verbatim}

So, all three of us agreed Thursday night on the shape of the result matrix.  We
figured if we had J x M coeffs and M x N basisvecs, then our
three-dimensional partial sums matrix ought to be J x M x
N.\footnote{Given J ``objects'' in the coefficient matrix, we should
  be able to reproduce J partial sums, right?  This is what we were
  thinking at the time.}  However,
we realized on Friday that this doesn't make any sense if we only have
M basis vectors, though.  How can we have a partial sum with $>$M terms
if the basis is M-dimensional?  We should only have as many partial
sums as there are terms, no?

In line with this line of reasoning, we changed the shape of our
placeholder zero matrix (the one that would eventually hold all the
partial sums) to be M x J x N.\footnote{Our code looks like this
  now: \texttt{result\_shape=len(basisvecs), len(coeffs),
    len(basisvecs.T)}} After reloading the module, we found that these
dimensions no longer gave us an error and produced the matrix we had
calculated previously.  Hooray!

\subsection{Uh-oh}
\label{sec:99problems}

So, our function takes up a lot of memory apparently, and also freezes
computers and hates Ninjali.  Even when she tries to be a reasonable
person and save the output.

To solve this, we cut the big ass partial sums matrix so that it only
includes up to the 200th order partial sum.  That should be enough in
order to recreate our spectra reasonably.  (Fingers crossed.)



\section{\texttt{res.residuals()}}
\label{sec:residuals}
\textsc{Leaving} pca.py, we now turn to our troublesome resids.py
module.  Many of these functions appear simple, yet they have been the
source of lots of trouble.

The first stumbling block is the foundation of the script in a way.
Though it isn't frequently called by the other functions, it
calculates the raw residual for us.

We knew that this function worked the way we wanted it to with two
exceptions.  Each subsection of this chapter describes the two
different problems and our approaches to them.

\subsection{ZeroDivisionError}
\label{sec:zerodivision}
We wrote our percent error function so that when passed two matrices,
$act$ and $est$, it would return $(act - est)/act$.  This is all fine
and good, unless any entry in the $act$ matrix is zero.

The good thing is that Python specifically tells us that the problem
is division by zero, rather than syntax or something like that.  The
bad news is that we might still want the process to work and just not
operate for that particular entry.

Right now, I'm thinking that we could ask Python to print the index
location of any and all problem entries so that we know where the
zeros are in $act$.  Implementation to come\ldots

\subsection{Floats vs. Ints}
\label{sec:float}

The other problem was that if our $act$ and $est$ matrices were
composed entirely of integer entries, then the difference would result in
an array of ints and thus the quotient would be computed in the
strange way that Python computes integer quotients.  Namely, $2/3 = 0$,
which is clearly not what we want\ldots

We didn't run into this problem because we were primarily handling
flux values and other things that have decimals, but just to be safe,
we didn't want to run into this awkward error.

First we assumed there was a function called \verb|np.float| that
would convert all the entries in $act$ to floats, but while there was
a function so named, it didn't do what we wanted it to.

It turns out that you can pass \verb|np.array()| the kwarg dtype,
though, and change the data type of all the entries that way.  Our
code now has the line

\begin{verbatim}
    act = np.array(act, dtype=float)
\end{verbatim}
to fix the problem.  We tested this on our $basisvecs$ matrix and got
the desired result.  The only problem is that our method creates a new
array, which is okay for now, but we would ideally not do that for
memory conservation. 


\section{\texttt{res.maxresidual()}}
\label{sec:maxresidual}
\begin{verbatim}
"""Finds the maximum (unsigned) percent error. By default, the array is
flattened, but set axis=0 for maxing along columns and axis=1 for the
rows."""
\end{verbatim}

I just wanted to test this function to make sure I understood the axes
correctly, to be honest.  We used a carefully engineered test matrix,

\[test= \left[ \begin{array}{ccc}
1 & 2 & 15 \\
-16 & 5 & 6 \\
7 & 8 & 9 \\
10 & 11 & -15
\end{array} \right]. \]

For the different kwarg axis values, I expected the outputs found in
Table \ref{tab:axes}.

\begin{table}
  \centering
  \begin{tabular}[c]{|c|c|}
    \hline\hline
    Axis & \verb|maxresidual(test, axis)| \\
    \hline \\ [-1.5ex]
    None & 15 \\ [1ex]
    0 & array([10, 11, 15]) \\ [1ex]
    1 & array([15, 6, 9, 11]) \\ [1ex]
    \hline
  \end{tabular}
  \caption{The expected outputs of \texttt{res.maxresidual(test,
      axis)}}
\label{tab:axes}
\end{table}
% Figure out how to get column borders in TeX!!!

So, these were not the outputs returned by the function!  But that's
okay because I was just being dumb; even though it was right there in
the docstring, I forgot that our maxresidual function took the
absolute value of our matrix before plucking out the maximum values.

With this in mind, the real outputs (found in Table \ref{tab:axes2}) make 100\% sense.  Our res.maxresidual()
works just the way we want it to.

\begin{table}
  \centering
  \begin{tabular}[c]{|c|c|}
    \hline\hline
    Axis & \verb|maxresidual(test, axis)| \\
    \hline \\ [-1.5ex]
    None & 16 \\ [1ex]
    0 & array([16, 11, 15]) \\ [1ex]
    1 & array([15, 16, 9, 15]) \\ [1ex]
    \hline
  \end{tabular}
  \caption{The actual outputs of \texttt{res.maxresidual(test,
      axis)}}
\label{tab:axes2}
\end{table}


\section{\texttt{res.graphmaxresiduals()}}
\label{sec:graphmaxresiduals}

Even though we originally thought it unnecessary to unit test our
graphing functions, we did find a small bug\footnote{Thanks log
  scaling\ldots} in \verb|graphresiduals()| so it's not a bad idea to
check everything.

Here's a rundown of what the function does.  It takes in the
three-dimensional matrix of residuals, \textit{resids}, which has the
$N^{th}$ order reconstruction of the spectral data along axis0, the list of
flux values for a given day along axis1, and the flux values for a
given wavelength along axis 3.  It also takes in the array of day
numbers, \textit{dayarr}, and some kwargs but they're not the main
focus here.

First our function takes the maximum residuals along axis 2, which
means it pulls out a single maximum residual for each day, for each
value of N. This creates a two-dimensional matrix where axis0
represents N and axis1 represents the different days -- in our code,
this is \textit{maxbyday}.

Then our function does either of two things:
\begin{enumerate}
\item If it is passed a value for the day kwarg, it will slice
  \textit{maxbyday} and plot the maximum residual values for that day
  as a function of N.
\item If day=None, our function will max along axis 1 of \textit{maybyday}.
  This action is equivalent to taking the maximum residual value
  across all of the days at a given N.  These \"{u}ber maximum residuals
  are plotted as a function of N.
\end{enumerate}

So to test this function, we created a three-dimensional matrix,

\[test= \left[ \begin{array}[c]{c}
    
    \left[ \begin{array}[c]{cccccc}
        1 & -2 & 3 & 4 & -5 & 6 \\
        -6 & 7 & 8 & 9 & 10 & -11 \\
        1 & 2 & -3 & 4 & 5 & 6
        \end{array} \right] \\

      \\

    \left[ \begin{array}[c]{cccccc}
        11 & 12 & 13 & 14 & -15 & 16 \\
        7 & 8 & 9 & 10 & 11 & 12 \\
        -16 & 12 & 9 & -2 & 5 & -8 \\
        \end{array} \right] \\

      \\

    \left[ \begin{array}[c]{cccccc}
        4 & 2 & 7 & 9 & -12 & 0 \\
        -3 & 27 & 9 & 21 & 14 & 7 \\
        5 & 10 & 15 & -20 & 37 & 2 \\
    \end{array} \right]

\end{array} \right]. \]


We also had to make another array that would substitute as our 'day
array' for indexing purposes.  Because test.shape = (3,3,6),
\textit{dayarr}, our array of 'days,' has the shape (3,).  Arbitrarily
I chose \[dayarr=\left[\begin{array}{ccc} -1 & 0 & 1 \end{array}\right]. \]

If we do res.graphmaxresiduals(test, dayarr, day=-1), we would expect
a plot of the points (0, 6), (1, 16), and (2, 12).  Similarly, if we set
day=0, we would get a plot of the points (0, 11), (1, 12), and (2, 27).
Just for completeness' sake, day=1 would yield (0, 6), (1, 16), and
(2, 37).\footnote{Note that because our function calls \texttt{day\_idx()} from our
specs.py module, graphmaxresiduals(test, dayarr, day=2) would give us
an IndexError.  You want to pass graphmaxresiduals() the values in \emph{dayarr}.}

Now, we created the function for the purpose of examining our
residuals, so by default it plots these points on a log scale, but I
momentarily commented the scaling part out just to make sure the
points look like the ones I expect.  Everything checks out.

If we were to ignore the day kwarg and use the default day=None, then
we would expect our function to plot (0, 11), (1, 16), and (2, 37).
This was exactly what our function returned.\footnote{Also, note that
  this function also returns the y-values that it plots as an array,
  in case you would like to see the numbers and perhaps do something with them.}


\section{\texttt{res.graphresiduals()}}
\label{sec:graphmaxresiduals}

This function plots the spectrum of residuals for a specified day from
a specified Nth order reconstruction of the data.  Again, we used our
three-dimensional \textit{test} matrix,

\[ \left[ \begin{array}[c]{c}
    
    \left[ \begin{array}[c]{cccccc}
        1 & -2 & 3 & 4 & -5 & 6 \\
        -6 & 7 & 8 & 9 & 10 & -11 \\
        1 & 2 & -3 & 4 & 5 & 6
        \end{array} \right] \\

      \\

    \left[ \begin{array}[c]{cccccc}
        11 & 12 & 13 & 14 & -15 & 16 \\
        7 & 8 & 9 & 10 & 11 & 12 \\
        -16 & 12 & 9 & -2 & 5 & -8 \\
        \end{array} \right] \\

      \\

    \left[ \begin{array}[c]{cccccc}
        4 & 2 & 7 & 9 & -12 & 0 \\
        -3 & 27 & 9 & 21 & 14 & 7 \\
        5 & 10 & 15 & -20 & 37 & 2 \\
    \end{array} \right]

\end{array} \right], \]

as our resids, and \[\left[\begin{array}{ccc} -1 & 0 &
    1 \end{array}\right] \] for our \textit{dayarr}.  Because this
function plots against an array of 'wavelengths' as x-values, we
created another array, $lams$, with the shape (6,):

\[ lams= \left[ \begin{array}{cccccc}
    100 & 200 & 300 & 400 & 500 & 600 \\
\end{array} \right]. \]

We expect \verb|res.graphresiduals(day=-1, N=0, test, lams, dayarr)|
to produce the array
\[ \left[ \begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6 \\
\end{array} \right] \]

and plot these as the y-values vs. the entries of $lams$ so that you
get the points (100, 1), (200, 2),\ldots, (600, 6) on log-scaled axes.

Were you instead to pass the arguments day=1 and N=2, you'd plot (100,
5), (200, 10), (300, 15), (400, 20), (500, 37), and (600,
2).\footnote{Again, remember that the day you enter in the argument
  list is \emph{not} the index of anything in the array!  The day\_idx
  function works behind the scenes so that you don't have to worry
  about indexing.}  Essentially, our function slices the
three-dimensional matrix of all the residuals to pick out the row of
residual values that interests you and then plots the absolute value
of that row.  This is exactly what we want it to do.

\section{\texttt{res.residuals()} II}
\label{sec:residuals2}

So, we tested our residuals function before and it was satisfactory.
See Section \ref{sec:residuals} for a refresher of our resolution of
any issues.  After attempting to analyze these residuals in plots,
however, we realized that we were getting (what we thought to be) high
residual values, particularly for the epochs way before peak
brightness.  We don't know if this is due to the actual shape of the
spectra for these epochs, or if this is due to the very small flux
values.  For example, day -20 has fluxes on the order of $10^{-38}$.
Dividing a very small number by this will give us a pretty large
number, so we can't exactly take the outputs of \verb|res.residuals()|
at face value.

Instead we're going to introduce a scale kwarg that allows us to
divide the difference $act-est$ by an input scale factor, which we
intend to be the ``amplitude'' of the centered spectrum.\footnote{By
  ``amplitude'' we mean the difference between the maximum flux value
  and the minimum for that day.}

Since we modified our code, we have to make sure it has retained its
old functionality while being able to do new tricks.  Let's start by
making sure the old tests (found above and in Anjali's
testresids.tex) for this function still work.  We used her original
$act$ and $est$ matrices,

\[act=\left[\begin{array}{cccc}
    1 & 4 & 7 & 9 \\
    11 & 21 & 3 & 4 \\
    6 & 8 & 1 & 42 \\
    5 & 36 & 6 & 17
\end{array} \right] \]

and
\[est=\left[\begin{array}{c}

    \left[\begin{array}{cccc}
      2 & 5 & 8 & 9 \\
      13 & 20 & 3 & 3 \\
      5 & 7 & 1 & 42 \\
      4 & 35 & 6 & 16
    \end{array} \right] \\

  \\

  \left[\begin{array}{cccc}
      1 & 4 & 7 & 9 \\
      11 & 20 & 3 & 4 \\
      7 & 8 & 1 & 40 \\
      5 & 36 & 6 & 17
    \end{array} \right] \\

  \\

  \left[\begin{array}{cccc}
      1 & 3 & 6 & 9 \\
      11 & 21 & 3 & 4 \\
      6 & 8 & 1 & 42 \\
      5 & 36 & 6 & 17
    \end{array} \right]

\end{array}\right].\]


We put in integer arrays and got the expected residual array in
floats.  Now we have to check the bells and whistles.  If we try
\verb|res.residuals(act, est, scale=2)|, we should get a result with
the same shape as $est$, namely

\[result= \left[ \begin{array}{c}

    \left[\begin{array}{cccc}
        -0.5 & -0.5 & -0.5 & 0 \\
        -1 & 0.5 & 0 & 0.5 \\
        0.5 & 0.5 & 0 & 0 \\
        0.5 & 0.5 & 0 & 0.5
      \end{array} \right] \\

    \\

    \left[\begin{array}{cccc}
        0 & 0 & 0 & 0 \\
        0 & 0.5 & 0 & 0 \\
        -0.5 & 0 & 0 & 1 \\
        0 & 0 & 0 & 0
      \end{array} \right] \\

    \\

    \left[\begin{array}{cccc}
        0 & 0.5 & 0.5 & 0 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0
      \end{array} \right]

\end{array} \right]. \]

This was indeed the result that we obtained.  Interestingly enough,
when we tried scale=0, Python doesn't return its usual DivideByZero
error, but instead gives us a RuntimeWarning and returns a matrix of
$\pm inf$s and $nan$s.  I'm not sure why, since we're just dividing by
zero, but I thought it was cool\ldots 


\section{\texttt{pca.whiten()}}
\label{sec:whiten}

In addition to the scaling of the spectra, we also wanted to try
whitening the data matrix by columns \emph{and} rows in order to
remove any vertical offset that may have been screwing up our
approximations.  This means we needed to pass pca.whiten an $axis$
kwarg so that it knows along which axis to calculate and subtract off
the mean.

\subsection{Rows \emph{and} Columns\ldots}
\label{sec:rowsandcols}
Averaging along a specific axis was no problem, but reworking the
dimensions so that the subtraction works out was more difficult.  At
first we tried the lines

\begin{verbatim}
if axis==0:
    whiteneddata = data[:,] - meandata
else:
    whiteneddata = data[,:] - meandata
\end{verbatim}

in the hopes that the slicing from axis=0 would work for axis=1.
However, this method returned a SyntaxError for the second
if-statement.  We even tried \verb|data[ ,:]| (as opposed to
\verb|data[,:]|) but that still failed.

Instead, we decided to use the same notation, but operate on the
transpose of the relevant matrices:

\begin{verbatim}
elif axis==1:
    whiteneddata = (data.T[:,] - meandata).T
else:
    print 'Ruh roh, this function only works for two-dimensional matrices.'
\end{verbatim}

Now, we kind of wanted to figure out a way to make this function work
for an N-dimensional data matrix, but for now this will do.  Using a
modified version of our $act$ matrix,

\[haha= \left[
  \begin{array}{cccc}
    1 & 4 & 7 & 9 \\
    11 & 21 & 3 & 4 \\
    6 & 8 & 1 & 42 \\
    5 & 36 & 6 & 17 \\
    -1 & 3 & 10 & -5
  \end{array} \right] \]

we predicted some outputs.  Along axis 0 the mean
is
\[\left[\begin{array}[c]{cccc}
   4.4 & 14.4 & 5.4 & 13.4
\end{array} \right] \]
so we expect \verb|pca.whiten(haha, axis=0)| to return
\[ \left[
  \begin{array}{cccc}
    -3.4 & -10.4 & 1.6 & -4.4 \\
    6.6 & 6.6 & -2.4 & -9.4 \\
    1.6 & -6.4 & -4.4 & 28.6 \\
    0.6 & 21.6 & 0.6 & 3.6 \\
    -5.4 & -11.4 & 4.6 & -18.4
  \end{array} \right]. \]

Yay!  Our function worked the way we wanted to for axis=0.  Now, we
expect \verb|pca.whiten(haha, axis=1)| to return
\[ \left[
  \begin{array}{cccc}
    -4.25 & -1.25 & 1.75 & 3.75 \\
    1.25 & 11.25 & -6.75 & -5.75 \\
    -8.25 & -6.25 & -13.25 & 27.75 \\
    -11 & 20 & -10 & 1 \\
    -2.75 & 1.25 & 8.25 & -6.75
  \end{array} \right]. \]
Double yay!  Still works.

\subsection{Wait, \texttt{np.mean()}?}
\label{sec:mean}

\emph{NB: This function only works as intended on two-dimensional
  matrices.}  It occured to me though, that as long as you passed our
whitening function axis=0 or axis=1 as a kwarg it would still do
\emph{something}, even to an ndarray of a bigillion dimensions. I was curious about how
plain old \verb|np.mean()| would work on the outermost axis of an ndarray with
$n\geq3$.

Using a new matrix I created specifically for this portion of the test
(ie, the means are easy to calculate by inspection), I began to explore
averaging over ndarrays.  Here's the new matrix:

\[a=\left[
  \begin{array}{c}
    \left[\begin{array}[c]{ccc}
      0.1 & 0.2 & 0.3 \\
      0.2 & 0.3 & 0.4
    \end{array} \right] \\

  \\

  \left[\begin{array}[c]{ccc}
      0.4 & 0.4 & 0.4 \\
      0.7 & 0.6 & 0.8
  \end{array} \right]
  \end{array} \right].
\]

Before I tried to average along axis=0, I expected
\verb|np.mean(a, axis=0)| to flatten each two-dimensional slice along
the $0^{th}$ axis, and so return
\[array(\left[
\begin{array}{cc}
  0.25, & 0.55
\end{array} \right]),
\]
which is just the mean of all the elements of $a[0]$ and $a[1]$.

Instead, what the function actually returned was
\[\left[
  \begin{array}[c]{ccc}
    0.25 & 0.3 & 0.35 \\
    0.45 & 0.45 & 0.6
  \end{array}
  \label{arr:axis0}
\right].\]



I stared at the output for a while and couldn't figure out what it
meant, so I started fiddling with the other axes to see if they lent
me any insight into the workings of NumPy's mean function.

First, I moved onto axis=1.  I expected that this would average
``along the columns'' of each matrix, which, I later realized, would
be equivalent to doing \verb|np.mean(a[i,:,j])| for $i=[0, 1]$ and
$j=[0, 1, 2]$. My expectation was to receive the output

\[\left[
  \begin{array}[c]{ccc}
    0.15 & 0.25 & 0.35 \\
    0.55 & 0.5 & 0.6
  \end{array}
\right].\]

Luckily, this was actually what \verb|np.mean(a, axis=1)| returned.
Similarly, I expected axis=2 to average ``along the rows'' of $a$ and
return
\[\left[
  \begin{array}[c]{cc}
    0.2 & 0.3 \\
    0.4 & 0.7
  \end{array}
\right],\]
which it did.

Investigating the more familiar axes revealed to me how
\verb|np.mean()| uses the axis kwarg.  The function slices along all
the other axes \emph{except} the one specified, instead using a colon
as a sort of placeholder index.  If my conclusion is right, then I
should be able to obtain the entries of \verb|np.mean(a, axis=0)| by
computing \verb|np.mean(a[:,i,j])| for $i=[0, 1]$ and $j=[0, 1, 2]$.
I've included all of my predictions in Table
\ref{tab:axis0predictions}.

\begin{table}
  \centering
  \begin{tabular}{|c|l|}
    \hline\hline
    $(i,j)$ & \verb|mean(a[:,i,j])| \\
    \hline \\ [-1.5ex]
    (0, 0) & 0.25 \\
    (0, 1) & 0.3 \\
    (0, 2) & 0.35\\ [0.5ex] \hline
    \\ [-1.75ex]
    (1, 0) & 0.45 \\
    (1, 1) & 0.45 \\
    (1, 2) & 0.6 \\ [0.5ex]
    \hline\hline
  \end{tabular}
  \caption{Expected entry computations for \texttt{np.mean(a,
      axis=0)}.}
\label{tab:axis0predictions}
\end{table}

Looking back at the actual output from axis=0, we see that these are
indeed the same entries.  It took me way too long to figure this out,
but I'm glad I did.

\subsection{Rows and Columns, cont.}
\label{sec:rowsandcols2}

So, what does this mean in terms of our \verb|pca.whiten()| function,
the only reason this section exists?  Coincidentally, the $a$
matrix worked just because of the shape, I think, but when we try
axis=1, the function barfs, so\ldots yeah.


\section{\texttt{pca.scale()}}
\label{sec:scale}

Remember how we were talking about scaling each spectrum so that when
we computed the residuals, we wouldn't have to divide by ridiculously
small numbers on the order of $10^{-38}$?  Yeah, so we created a
function for that and called it \verb|pca.scale|.  We designed it to
operate on two-dimensional arrays in order to normalize all the
spectra at once.

\subsection{RuntimeWho?}
\label{sec:scalewarning}
So, we ran into unexpected issues with our straight-forward, five-line
\verb|pca.scale| function.  Originally we had it doing division twice,
but because the computer has to compute the reciprocals of the
divisors and then do scalar multiplication, we modified the code so
that division only appeared once, like so:

\begin{verbatim}
    amplitude = np.zeros((len(data), 1))
    for i in np.arange(len(data)):
        amplitude[i] = np.amax(data[i]) - np.amin(data[i])
        factor = scalar/amplitude

    return data*factor, factor.
\end{verbatim}

After reloading the pca.py module, we tried pca.scale(specs, 2.), where
specs was our spectral data matrix and we received
\begin{verbatim}
RuntimeWarning: divide by zero encountered in divide
    factor = scalar/amplitude
\end{verbatim}
instead of the two desired matrices.

Now, we were confused by this, so we reloaded pca.py again and it
worked fine\ldots.  This issue is still unresolved, but not a big
problem, since things work.

\subsection{A Toy Problem: using $\cos(\theta)$}
\label{sec:cos}

So, to make sure that \verb|pca.scale()| and \verb|pca.whiten()| work
together, we made a plot of $4\cos(\theta) + 5$ for $0 \leq \theta <
6\pi$, centered it, and then scaled it to a unit max-min difference of
2, which is to say an amplitude of 1.  At each step we plotted our
results.

\begin{center}
  \includegraphics[width=13cm]{costest.pdf}  
\end{center}

As we had hoped, we ended up with plots of $4\cos(\theta)$ after
centering and $\cos(\theta)$ after rescaling.

\begin{center}
HOORAY.  
\end{center}

\end{document}
