\documentclass{article}

\title{Data Mining in Galaxy Spectra: \\ a.k.a., What Ninjali \& I Did This Summer}
\author{Nicole Loncke}
\date{\today}

\usepackage{amsmath}
\usepackage{multicol}

\newcommand{\mtx}[1]{\mathbf{#1}}
\newcommand{\defn}[1]{\textbf{#1}}

\begin{document}

\maketitle

\tableofcontents

\begin{abstract}
The observed spectrum of a galaxy is not always consistent with what
we expect.  In some of these instances, we may be seeing the combined
spectra of (i) a run-of-the-mill galaxy and (ii) a bright transient
event that happened to occur in that galaxy at the time of
observation.

By modeling such spectra as a galaxy-plus-supernova combination, an
ongoing project at AMNH and NYU has uncovered previously unknown
supernovae buried in the SDSS catalog of galaxy spectra.  For every
spectrum in the catalog, their code cycles through thousands of SN
spectra, adds each to a fiducial galaxy spectrum, and selects the
best-fit.

While effective, this brute-force method is inefficient. Using
theoretical SN1a spectra as a testbed, we investigate whether
supernova spectra can be well-represented as combinations of a much
smaller number of basis spectra.  If so, we will see that the fitting
step could be bypassed altogether.  Our talk will outline the proposed
technique and our progress to date.The observed spectrum of a galaxy
is not always consistent with what we expect.  In some of these
instances, we may be seeing the combined spectra of (i) a
run-of-the-mill galaxy and (ii) a bright transient event that happened
to occur in that galaxy at the time of observation.

By modeling such spectra as a galaxy-plus-supernova combination, an
ongoing project at AMNH and NYU has uncovered previously unknown
supernovae buried in the SDSS catalog of galaxy spectra.  For every
spectrum in the catalog, their code cycles through thousands of SN
spectra, adds each to a fiducial galaxy spectrum, and selects the
best-fit.

While effective, this brute-force method is inefficient. Using
theoretical SN1a spectra as a testbed, we investigate whether
supernova spectra can be well-represented as combinations of a much
smaller number of basis spectra.  If so, we will see that the fitting
step could be bypassed altogether.  Our talk will outline the proposed
technique and our progress to date.
  
\end{abstract}


\section{Thought Processes}
\label{sec:thinking}

Recover a reasonably small basis for type1a supernova spectra.

\vspace{5mm}

% \begin{multicols}{2}

  \subsection{A Toy Problem}
  \label{sec:hsiao}
  We decided to apply the same fundamental technique to a simpler
  problem.  Can we recover a basis set of functions for a single
  supernova that can reproduce the spectrum of a single day from that
  supernova?

  Since we were unfamiliar with the processes that we would be
  implementing, we also decided to make things simpler yet: rather
  than using the spectral data from a real, observed type 1a SN, we
  instead used ``fake'' supernova data generated by researcher Eric
  Hsiao in 2007.  \textsc{He created an algorithm based on the stretch
    of the supernova (?) and the days after peak brightness that
    produced these flux values.}

  \subsection{PCA}
  \label{sec:pca}

  Thus far we have mentioned the notion of a basis for the spectra a
  few times without explaining how we could possibly obtain such a set
  of functions.  This project required that we become familiar with a
  process called \defn{Principal Component Analysis}.

  What PCA does is uncover whether or not the data effectively lie in
  a plane of lower dimensionality.  It does this by constructing a
  basis that spans the data-space and whose elements decrease
  monotonically by ``relevance,'' so to speak.  The first element of
  this basis, or the first \defn{principal component}, ``points'' in
  the direction along which the data spreads, or varies, the
  most.  Similarly, the second principal component indicates the
  direction that contains the second most variance within the data
  set.

  PCA provides as many basis vectors as are required to span the full
  dimensionality of the data, but only so many of the vectors are
  necessary to capture the most important features of the data.  How
  might we define a criterion for how many basis vectors we ought to
  include?

  **Should probably mention SVD at some point so that I can talk about
  the outputs**

  Back up, back up: SVD.  So PCA is actually based on a linear algebra
  algorithm called \defn{Singular Value Decomposition}, or
  \defn{SVD}.  We can think of it as the more general case of
  orthogonal diagonalization, a process by which a square matrix is
  factored into the product of three matrices 

  Singular values




% \end{multicols}



\section{Methodology}
\label{sec:procedure}

\begin{enumerate}
\item Rip the spectral data from Eric Hsiao's data template.
\item Normalize the ``amplitude'' of each specrtum.
\item Center the spectrum around zero, or \emph{whiten} the spectral
  data matrix by rows.
\item Apply SVD to the data matrix and combine the outputs to yield a
  matrix of basis vectors and coefficients for said basis vectors.
\item Regenerate the data matrix using the coefficient and basis
  vector matrices using up to $N$ basis vectors each time for $N = \#
  of samples$.
\item C
\end{enumerate}



\section{Conclusions}
\label{sec:conclusions}

Here's the part where we talk about what we found.

\end{document}
