\documentclass{article}
\usepackage{amsmath}

\begin{document}
\section{Coding and things}
\label{sec:intro}

Sooo...to begin with, we had a script that would spit out x-y ellipse
coordinates into a 24 by 2 array (or however many points you had)
given semi-major and -minor axes and a $\phi$-step.  We set the step to
$\pi/6$ and generated the 24 points.

We didn't however\ldots
\begin{enumerate}
\item normalize our $\vec{u}$ or $\vec{v}$ to be $\hat{u}$ and $\hat{v}$.
\item center our matrix by columns.  We used the blackbox
  \verb|numpy.center_matrix| which seemed to do the job just fine.  Whereas
  you averaged the columns, we used the blackbox to 
  subtracted the mean of the \textit{rows}.  We can pass it a kwarg so
  that it centers by rows, but as of now I'm not really sure what the
  significance of each method is\ldots.When we figure out the
  geometric significance of the mean we'll get back to you on why this matters!
\end{enumerate}

For now, we're creating our own function that whitens the ellipse
matrix by rows.\footnote{It can easily be modified to whiten by
  columns though. We shall compare this to the output of the
  numpy.center\_matrix to see what's up.}

Ahh!  Note to self: the \verb|numpy.center_matrix| function both zero
means the matrix \textit{and} makes each vector have unit standard
deviation.  Not exactly sure what this means, but upon inspection, the
resulting matrix is still zero meaned, so it should be all good\ldots

\section{WTF: Questions about life}
\label{sec:whydoestheuniversehateus}

\subsection{Whitening}
\label{whitening}
After several (I mean \textsc{several}) attempts at creating a
whitening function that computed the means of the columns or rows of a
matrix, we ran into an error that repeatedly said:
\begin{verbatim}
ERROR: Internal Python error in the inspect module.
Below is the traceback from this internal error.
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/IPython/core/ultratb.py",
line 756, in structured_traceback
    records = _fixed_getinnerframes(etb, context, tb_offset)
  File "/usr/lib/python2.7/dist-packages/IPython/core/ultratb.py",
line 242, in _fixed_getinnerframes
    records  = fix_frame_records_filenames(inspect.getinnerframes(etb,
context))
  File "/usr/lib/python2.7/inspect.py", line 1043, in getinnerframes
    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))
  File "/usr/lib/python2.7/inspect.py", line 1007, in getframeinfo
    lines, lnum = findsource(frame)
  File "/usr/lib/python2.7/inspect.py", line 580, in findsource
    if pat.match(lines[lnum]): break
IndexError: list index out of range

Unfortunately, your original traceback can not be constructed.
\end{verbatim}

This is extremely frustrating and has kept us here until 6:45. We must
be doing something wrong with our syntax, but we can't for the life of
us figure out what the hell is wrong.  The interpreter isn't much help
either :/.

Although we're extremely disappointed, we're just going to push on and
skip to the interpretation of the SVD outputs, which is the most
important part of this process, I think.

\subsection{Mean significance}
\label{mean}

The mean of the columns of our ellipse data matrix ought to be zero,
since our points are evenly distributed around the ellipse.  The value of one
$\vec{u}$-coordinate is canceled by the negative value of another in
the opposite quadrant and the same for $\vec{v}$.

This is in the $\vec{u}$-$\vec{v}$ plane, btw.  If we extrapolate to
the ambient 4-D space, the same holds.  Thinking about the mean more
generally, we see that it represents a translation of the center of
our ellipse.  Because our figure is centered at the origin of the
$\hat{x}$-$\hat{y}$ coordinate system, it hasn't been translated from
the standard Cartesian grid.

In regards to our real fake data--keep in mind, that we're only
handling Hsiao's template spectra--centering our matrix will be
important for reasons that are still unclear.  Will SVD/PCA not
recover a suitable basis if the data has been translated from some
origin?  Does the process know of any origin, actually?  Huh\ldots

\subsection{Choosing $\vec{u}$ and $\vec{v}$ for the ellipse}
Another lingering question: our choice of $\vec{u}$ and $\vec{v}$ seem
to be affecting our reduced dimension basis and therefore our
projected ellipse.  We did the same exact pca process several times
with a different 4-D plane each time and got wildly different results.

For example, when we choose the orthonormal $\vec{u}$ and $\vec{v}$
that you chose (with the original lengths of 13) everything worked
fine and we found a new set of two basis vectors that were orthogonal
to each other.\footnote{For some reason, these basis vectors
  aren't the same as the original vectors that we used to span the
  plane. Not sure why, exactly}

\textsc{This question has been solved!  You must either propagate a
  4-dimensional ellipse by creating linear combinations of two
  orthonormal basis vectors using the standard
  $\hat{x}$-$\hat{y}$ coordinates, or use plain old orthogonal vectors
  for your basis, but change the coordinates to reflect a rescaling of
  the components.  I would explain this a little bit better now, but
  Gabe wrote some notes, plus Anjali will undoubtedly clear up any
  confusion in the morning.  It's four minutes short of midnight.  Go
  to sleep.} 

\section{Using numpy.linalg.svd()}
\label{sec:svd}
So, we know that the process of singular value decomposition rewrites
a matrix as the product of three matrices, each of which tells us
something about the original matrix.  Typically, the decomposition is
written as
\begin{equation}
  \label{eq:svd1}
  A = U \Sigma V^{T}
\end{equation}

or alternatively,
\begin{equation}
  \label{eq:svd2}
  AV = U \Sigma
\end{equation}
where A is the matrix you want to decompose.  For our purposes, A is a
data matrix, meaning 

This section explains how to interpret the output of the numpy SVD
function in terms of PCA, or at least how I've been interpreting it,
which may or may not be the most useful way of doing so.

\subsection{Interpreting $\Sigma$}
\label{sec:sigma}
The second output matrix of the numpy SVD function, $\Sigma$ is a
pseudo-diagonal matrix that contains the singular values of the data
matrix A, or the square roots of the eigenvalues of $A^{T}A$.

\section{What to do next?}
\label{sec:instructions}

\begin{enumerate}
\item Cut out days -20 and -19, SVD the shit that's left, and then
  interpret the data outputs.  We should be finding the product of
  $U\Sigma$ in order to recover the coefficients of our samples with
  respect to the new basis found in the columns of V. \textbf{DONE.}
\item Plot
  \begin{math}
    \frac{True spec - Recovered Spec}{Truespec}
  \end{math}
  for various values of \textit{n}, where \textit{n} is the number of
  eigenvectors we use to reconstruct our data.  This is the residual!
\item write a function that will tell us how many days we need in
  order to achieve a maximum residual tolerance of $10^{-3}$ or
  something like that.
\item produce an array of the residual value you get from only
  including n functions (see Gabe's notes for elaboration)
\end{enumerate}

\section{Ideas}
\label{sec:ideas}

We've gotten to the point where we understand the configuration of
$U\Sigma$ pretty well, so we're up to the interpreting part.  To
discover any underlying structure, we plot the coordinates produced by
$U\Sigma$ against each other in ordered $(c_{1},c_{2})$ pairs.
Now...I get why this is relevant, but I'm trying to put my
understanding into words so I don't forget/realize that I don't
actually know what I think I know.  Here goes!

Each $(c_{1},c_{2})$ pair contains information about the same basis
vector (found in the columns of V) on different days.  If you were to
plot the first two rows of the coefficient matrix against each other,
so to speak, the first pair would represent the weighting of the first
basis vector on the first day against the weighting of the same basis
vector on the second day, the second pair would give you the weights
of the second vector on the first day and the second day, etc.

In short, each coefficient row of $U$ contains the weights of all the
vectors $\vec{v_{1}}, \vec{v_{2}},\ldots,\vec{v_{k}}$ found in the
first $k$ rows of $V^{T}$.

\section{Play-by-Play}
\label{sec:procedure}

The process of plotting all of these rows is very time consuming,
however, so we're writing a function that will plot and save these
figures for us to look at one at a time.

\ldots and making a robust function was too time-consuming.  We're
splitting up so that Anjali takes the residuals and I plow ahead, make
plots, and look at stuff.

We wrote a function to calculate the tolerance of an approximation!
It's not yet good enough to cycle through all the n ways of cutting $U$,
$\Sigma$, and $V^{T}$ yet, but maybe we'll get there.  Maybe.  Now
we're going to interpret us some plots.  Yeeeaaahhhhhhhh!!!!

We also created a function, \verb|reconstruct(specs, n)| that basically cuts our original, whitened
data matrix appropriately in order to reconstruct an n-dimensional
approximation of it.  Combined with \verb|residuals(whitespecs, recon_specs)| we can now take a look at our error margins relatively
quickly.

\subsection{Results?}
\label{sec:results}

So far we've made both risky and safe approximations.  For the n = 35
attempt, we saw a whopping maximum error of 147.56674.  And, no, that's not
even a percentage.  So then we tried something a bit more tame: n =
104, without cutting off the first two ``bad'' days.  Reassuringly, we
got a pretty low value.  Just so that we have a log of these numbers,
here's a handy dandy chart.
\begin{table}[r]
  \centering
  \begin{tabular}[c]{c||cc}
    \hline\hline
    N & Average residual & Largest residual value \\
    \hline \\
    106 & 1.4022588e-13 & 2.2195765e-09 \\
    105 & 1.4383019e-13 & 2.0904165e-09 \\
    104 & 2.0376491e-09 & 5.0975949e-06 \\
    103 & 3.2434754e-09 & 4.3732108e-06 \\
    102 & 8.0214531e-09 & 1.8203477e-05 \\
    101 & 3.3478537e-08 & 1.8572094e-04 \\
    100 & 4.7932189e-08 & 1.8930889e-04 \\
    99 & 7.1374826e-08 & 6.0613507e-04 \\
    98 & 1.3442393e-07 & 1.0923505e-03 \\
    35 & 9.2522540e-03 & 1.4756674e+02 \\
    \hline\hline
  \end{tabular}
  \caption{The average and maximum residuals of our n-dimensional
    approximation of the spectrum data.}
\label{tab:residuals}
\end{table}

\subsection{Results Pt. II}
\label{sec:results2}
So, after talking to Gabe we found out that the functions we created
were correct, but either weren't conducive to efficiency, or they
simply weren't what he was looking for.  Whatever.  So we're writing
some functions that treat the process as calculating the
\emph{elements} of the n-term sum instead of just calculating the nth
order approximation.  And then we're recursively computing the partial
sums by recursively adding on the contribution of each basis vector
because we do actually want all of the sums.

\end{document}
